{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6460d2b9d26b1e20",
   "metadata": {},
   "source": [
    "# Chapter 7: Advantage examples of Bayesian stats. \n",
    "\n",
    "Suggested topics: \n",
    "    - Bayesian Billiards game \n",
    "    - Rejecting outliers with MCMC \n",
    "Idea: create simulation of BB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69c4d099-5767-43aa-9198-15c39a54d8a1",
   "metadata": {},
   "source": [
    "## Bayesian Billiards\n",
    "A famous thought experiment comparing the Bayesian and frequentist approach is 'Bayesian billiards'.\n",
    "\n",
    "Suppose we have a rectangular billiard table with length $L$. We throw one black ball on the table, which will move around until it stops, at a random $x$-coordinate of the table, and call this $z$, $0\\leq z\\leq L$. For the purposes of this thought experiment, suppose we don't know $z$, and want to estimate it. We could do this by throwing $n$ white balls onto the table, and seeing whether they end up to the left, or to the right of the black ball. By counting how many are to each side, we can reasonably 'guess' the value of $z$. To make the calculation easier, suppose $L=1$. \n",
    "\n",
    "Note that one white ball is to the left of the black one with probability $z$, and to the right with probability $1-z$. The random variable \n",
    "$$\n",
    "X_i=\n",
    "\\begin{cases}\n",
    "1 &\\text{ if ball $i$ is to the left}\\\\\n",
    "0 &\\text{if ball $i$ is to the right}\n",
    "\\end{cases}\n",
    "$$\n",
    "is then a Bernoulli random variable with parameter $z$. \n",
    "The probability density is $f(X_i|Z=z)=(1-z)^{1-X_i}+z^{X_i}$. \n",
    "The observations $X_1,\\ldots,X_n$ are an i.i.d. sample, and the log-likelihood is\n",
    "$$\\ell(z)\n",
    "=\n",
    "\\log(1-z)\\left(n-\\sum_{i=1}^n X_i\\right)\n",
    "+\\log(z)\\sum_{i=1}^nX_i .\n",
    "$$\n",
    "Setting the derivative of the log-likelihood equal to 0 gives\n",
    "$$\n",
    "\\frac{\\text{d}\\ell(z)}{\\text{d}z}\n",
    "=\n",
    "-\\frac{n-\\sum_{i=1}^n X_i}{1-z}(1-z)\n",
    "+\n",
    "\\frac{\\sum_{i=1}^n X_i}{z}\n",
    "=\n",
    "0,\n",
    "$$\n",
    "which gives us the following the frequentist MLE:\n",
    "$$\n",
    "\\hat z_{\\text{MLE}}\n",
    "=\n",
    "\\frac{\\sum_{i=1}^n X_i}{n}=\\overline{X}.\n",
    "$$\n",
    "So if exactly half of the $n$ white balls lie to the right, the estimate is \n",
    "$\\hat z_{\\text{MLE}}=1/2,$\n",
    "which is to be expected. \n",
    "But if all of the balls lie to the right, the estimate becomes $\\hat z_{\\text{MLE}}=0$, which means that the black ball would lie on the left edge.\n",
    "Intuitively, however, we expect the ball to lie a little to the side, depending on $n$. \n",
    "\n",
    "Now consider the Bayesian framework. \n",
    "The black ball has an equal chance of being at each part of the table, so the prior of $Z$ is the uniform distribution between 0 and 1, which means $\\pi(z)=1$ for all $0\\leq z\\leq1$. \n",
    "The posterior distribution is then\n",
    "$$\n",
    "f(z|X_1,\\ldots,X_n )\n",
    "\\propto\n",
    "f(X_1,\\ldots,X_n|Z=z)\n",
    "\\pi(z)\n",
    "=z^y(1-z)^{n-y},\n",
    "$$\n",
    "where $y=\\sum_{i=1}^n X_i$ is the total number of balls that ended up on the left.\n",
    "\n",
    "This can be recognized as a $\\text{Beta}(y +1, n-y +1)$ distribution. \n",
    "From theory, we know that a $\\text{Beta}(\\alpha,\\beta)$ distribution has expectation $\\alpha/(\\alpha+\\beta)$.\n",
    "The MAP is therefore:\n",
    "$$\\hat z_{MAP}= \\frac{\\sum_{i=1}^n X_i+1}{n+2}.$$\n",
    "The difference between the MAP and MLE estimators can best be illustrated by thinking about an example: suppose $n=4$ balls have been added to the table, and all of them ended up on the right, so $y=0$. \n",
    "The frequentist method would suggest $\\hat z_{MLE}=0$, as discussed before. \n",
    "But the Bayesian method would give $\\hat z_{MAP}=1/6$, a little from the edge.\n",
    "To see how the estimators compare, let's simulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e5d9567-0a1a-461c-90a6-d242c877622b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#piece of code simulation Bayesian Billiards for different n, and comparing MAP and MLE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aa2edc0-d09b-4321-ad67-5fd7c9ae218c",
   "metadata": {},
   "source": [
    "Conclusion about simulation (MAP works better in this case)\n",
    "\n",
    "TODO:\n",
    "- discus where difference comes from\n",
    "- look for a better frequentist method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c50329c-16bc-45b8-b804-b005ae15cc44",
   "metadata": {},
   "source": [
    "## Rejecting Outliers with Markov Chain Monte Carlo simulations\n",
    "\n",
    "$\\def\\f{\\frac} \\def\\L{\\mathcal{L}}$\n",
    "<b> I will check for spelling/correct wording/layout and stuff later :)</b>\n",
    "\n",
    "When doing measurements of a certain physical relation, you would not expect that all points are within one standard deiviation of the theoretical relation. If you assume a big enough sample, we can assume a normal distribution (CLT) thus then the chance of getting a datapoint outside of the 1-$\\sigma$ region is about $32$%, so you would expect about a third of the measurements to be (at least) 1-$\\sigma$ away from the exact relation. However, as we go further and further away from the relation, we expect less and less datapoints there. One of these outliers, a datapoint very far away from the relation, can substantially impact the fit, because it can substatnially change the mean, for example. Hence, there is need to reduce the sensitivty of these outliers. We will do this by removing the outliers, since there is always a change on outliers. Outliers can have multiple causes, for example unmodeled experimental uncertainty or (rare) noice sources, for which aren't always able to account for. One process of removing outliers is by hand. However, as one could image, this is far from ideal, as it can be very subjective and hard to reproduce. \n",
    "\n",
    "Let us make a more systematic way of rejecting outliers. For simpliicity let us look at a straight line. Let $X_1, \\dots, X_n$ be an independent and identically distributed sample, and let $a = (q_1, \\dots, q_n)$ be a set op $n$ binary integers, where $q_i$ is $1$ if the $i$th datapoint is \"good\" and $q_i$ is $0$ if the $i$th datapoint is \"bad\"; an outlier. Furthermore, let $P_b$ be the prior probability that a datapoint is bad and let $(Y_b, V_b)$ be the mean and variance of the distribution of bad points. Finally, let $I$ be all other information. Note that these extra parameters will be latere marginalized out, we do not need to worry about the fact we have more datapoints than our \"actual\" datapoints. \n",
    "\n",
    "<b> Generative model: </b> a parameterized, quantitative description of statisciatcal procedure that could reasonably hace generated the dataset you have.\n",
    "\n",
    "Let $p_g$ be the generative model for the \"good\" datapoints and $p_b$ be the generative model for the \"bad\" datapoints. $p$ is generaly given by: \n",
    "- $p$: the frequency distribution (eq 9)\n",
    "$$\n",
    "p(y_i | x_i, \\sigma_{y_i}, m, b) = \\f{1}{\\sqrt{2\\pi \\sigma_{y_i}^2}} \\exp\\left(-\\f{[y_i - Y_i]^2}{2\\sigma^2_{y_i}}\\right) ,\n",
    "$$\n",
    "where $Y_i$ is the oucome of the model. For example, for a linear model with slope $m$ and zeropoint $(x=0)$ $b$, there holds $Y_i = mx_i + b$. \n",
    "\n",
    "We look for the model that maximizes the probability of the observed data, given the model and other factors. Hence, we will calculate the likelihood. Let $\\hat{\\theta}$ be all the model parameters of the model. Then, we can see: \n",
    "\n",
    "$$\n",
    "\\L = p(X_1, \\dots, X_n | \\hat{\\theta}, q_1, \\dots, q_n, Y_b, V_b, I)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\L = \\prod_{i=1}^n p(X_i | \\hat{\\theta}, q_i, n, Y_b, V_b, I)\n",
    "$$\n",
    "\n",
    "(since the $X_i$ are iid)\n",
    "\n",
    "$$\n",
    "\\mathcal{L} = \\prod_{i = 1}^n  \\left[p_g(X_1, \\dots X_n | \\hat{\\theta}, I)\\right]^{q_i} \\left[p_b(X_1, \\dots X_n | Y_b, V_b, I)\\right]^{1-q_i} \n",
    "$$\n",
    "\n",
    "As we can see that for good datapoints, $q_i = 1$, thus the second term will be unity, and for bad datapoints $q_i = 0$ so the first term will equal 1.\n",
    "\n",
    "$$\n",
    "%\\mathcal{L} = \\prod_{i = 1}^n  \\left[ \\frac{1}{\\sqrt{2\\pi \\left[V_b + \\sigma^2_{X_i}\\right]}} \\exp \\left(-\\frac{[X_i - my_i - b]^2}{2\\sigma_{X_i}^2} \\right)\\right]^{q_i}  \\cdot  \\left[ \\frac{1}{\\sqrt{2\\pi \\left[V_b + \\sigma^2_{X_i}\\right]}} \\exp\\left(-\\frac{[X_i - Y_b]^2}{2[V_b + \\sigma_{X_i}^2]}\\right)\\right]^{1-q_i}\n",
    "$$\n",
    "\n",
    "Now, as we permit the rejection of some of our data, we must make a prior probabily on our values of $q_i$ that penelized each rejection, as otherwise we keep rejecting.... \n",
    "\n",
    "$$ \n",
    "p (\\hat{\\theta}, \\{q_i\\}^n_{i=1}, P_b, Y_b, V_b | I) = p(\\{q_i\\}^n_{i=1} | P_b, I) p (\\hat{\\theta}, P_b, Y_b, V_b | I) \n",
    "$$\n",
    "with \n",
    "$$ \n",
    "p(\\{q_i\\}^n_{i=1} | P_b, I) = \\prod_{i=1}^n [1-P_b]^{q_i} P_b^{[1-q_i]}\n",
    "$$\n",
    "the binomial probabiltiy of the particular sequanece $\\{q_i\\}^n_{i=1}$. \n",
    "\n",
    "The prior on the parameters $(P_b, Y_b, V_b)$ should be made without looking at the data. You can either set the prior based on prior knowlege or let the prior be somewhat uninformative, for example $P_b \\in [0,1]$. \n",
    "\n",
    "- tell something about how a Gaussian is not really the correct model (p. 12-13) but ....\n",
    "\n",
    "Posterior probability distribution function: \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "- p. 11 Hoggs et al: \"the marginalization will require that we have a measure on our parameters (integrals require measures) and that measure is provided by a prior.\" -> marginalization requires Bayesian stats\n",
    "\n",
    "\n",
    "##### used sources:\n",
    "\n",
    "Data analysis recipes: Fitting a model to data, Hogg et al, 2011 (from BS), chapter 3\n",
    "\n",
    "\n",
    "##### possible sources: \n",
    "\n",
    "https://www.astroml.org/book_figures/chapter8/fig_outlier_rejection.html \n",
    "\n",
    "https://www.stat.cmu.edu/technometrics/59-69/VOL-02-02/v0202123.pdf\n",
    "\n",
    "https://d-nb.info/1221556185/34\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c30a8c1-1d1a-42ca-8f08-32d6067278f4",
   "metadata": {},
   "source": [
    "### Sigma Clipping"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
